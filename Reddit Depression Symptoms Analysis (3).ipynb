{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
},
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reddit Depression Final Project\n",
        "Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ],
      "metadata": {
        "id": "9jFvbbC6VtZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install happiestfuntokenizing\n"
      ],
      "metadata": {
        "id": "afMP20tOP301",
        "outputId": "ec37fb64-f00a-4656-804d-a31682b28522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: happiestfuntokenizing in /usr/local/lib/python3.10/dist-packages (0.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "FoBxKQ_OVl-j",
        "outputId": "b7acfccf-c0fe-498b-911c-078f22f05b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer\n",
        "import spacy\n",
        "import gc\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import string\n",
        "import itertools\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = 'drive/MyDrive/comp_ling'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "rcMOTL7mV9T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load(filename):\n",
        "  \"\"\"Load pickles. FILEPATH defined above. filename is the reddit depression data pkl\"\"\"\n",
        "  data = pd.read_pickle(f'{FILEPATH}/{filename}.pkl')\n",
        "  return data"
      ],
      "metadata": {
        "id": "icnzto8GWFlb"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_generation(data, subreddit_category_mapping):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  raw data and a mapping from subreddit to symptom category\n",
        "  defined by Liu et al.\n",
        "\n",
        "  Function:\n",
        "  Creates control dataset based on Gkotsis et al. (2017) framework\n",
        "  Adds 'Label' and 'Category' features to Dataframe\n",
        "\n",
        "  Returns:\n",
        "  Dataframe with added columns\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate control dataset\n",
        "  # Create a dictionary where keys are authors and values are the first time (created_utc) they made a post in any of the depression subreddits\n",
        "  author_date = data[data['subreddit'].isin(depression_subreddits)].groupby('author')['created_utc'].min().to_dict()\n",
        "\n",
        "  # convert 180 days to utc\n",
        "  seconds_in_180_days = 180 * 24 * 60 * 60\n",
        "\n",
        "  # Create \"Label\" feature that assigns value of 1 to posts in a depression category and 0 to posts in the control category and Nan if neither\n",
        "  # define control categoy as all posts from non-depression subreddits that have a created_utc at least 180 days before the date listed in author-date pairs\n",
        "  data['Label'] = np.where(\n",
        "      (~data['subreddit'].isin(depression_subreddits)) &\n",
        "      (data['author'].isin(author_date)) &\n",
        "      (data['created_utc'] <= data['author'].map(author_date) - seconds_in_180_days),\n",
        "      0,  # Label as 0 if the condition is met\n",
        "      np.where(\n",
        "          data['subreddit'].isin(depression_subreddits),\n",
        "          1,  # Label as 1 if subreddit is in depression_subreddits\n",
        "          np.nan  # Label as NaN if neither condition is met\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Create \"Category\" feature that labels \"Control\" for all control posts and the\n",
        "  # depression symptom (eg. \"Anger\") for the deppression-labelled posts based on their subreddits\n",
        "  data['Category'] = np.where(data['Label'] == 0, 'Control', data['subreddit'].map(subreddit_category_mapping))\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "Wpw9kJiras4B"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of depression subreddits in the paper\n",
        "depression_subreddits = [\"Anger\",\n",
        "    \"anhedonia\", \"DeadBedrooms\",\n",
        "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
        "    \"DecisionMaking\", \"shouldi\",\n",
        "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
        "    \"chronicfatigue\", \"Fatigue\",\n",
        "    \"ForeverAlone\", \"lonely\",\n",
        "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
        "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
        "    \"insomnia\", \"sleep\",\n",
        "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
        "    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n",
        "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
        "]\n",
        "\n",
        "# depression symptoms; commented out symptoms not included in analysis\n",
        "depression_symptoms = [\"Anger\",\n",
        "                         \"Anhedonia\",\n",
        "                         \"Anxiety\",\n",
        "                         #\"Concentration deficit\",\n",
        "                         \"Disordered eating\",\n",
        "                         #\"Fatigue\",\n",
        "                         \"Loneliness\",\n",
        "                         \"Sad mood\",\n",
        "                         \"Self-loathing\",\n",
        "                         \"Sleep problem\",\n",
        "                         \"Somatic complaint\",\n",
        "                         #\"Suicidal thoughts and attempts\",\n",
        "                         \"Worthlessness\"]\n",
        "\n",
        "# Dictionary of subreddit to category\n",
        "subreddit_category_mapping = {\n",
        "    \"Anger\": \"Anger\",\n",
        "    \"anhedonia\": \"Anhedonia\",\n",
        "    \"DeadBedrooms\": \"Anhedonia\",\n",
        "    \"AnxietyDepression\": \"Anxiety\",\n",
        "    \"Anxiety\": \"Anxiety\",\n",
        "    \"HealthAnxiety\": \"Anxiety\",\n",
        "    \"PanicAttack\": \"Anxiety\",\n",
        "    \"DecisionMaking\": \"Concentration deficit\",\n",
        "    \"shouldi\": \"Concentration deficit\",\n",
        "    \"bingeeating\": \"Disordered eating\",\n",
        "    \"BingeEatingDisorder\": \"Disordered eating\",\n",
        "    \"EatingDisorders\": \"Disordered eating\",\n",
        "    \"eating_disorders\": \"Disordered eating\",\n",
        "    \"EDAnonymous\": \"Disordered eating\",\n",
        "    \"chronicfatigue\": \"Fatigue\",\n",
        "    \"Fatigue\": \"Fatigue\",\n",
        "    \"ForeverAlone\": \"Loneliness\",\n",
        "    \"lonely\": \"Loneliness\",\n",
        "    \"cry\": \"Sad mood\",\n",
        "    \"grief\": \"Sad mood\",\n",
        "    \"sad\": \"Sad mood\",\n",
        "    \"Sadness\": \"Sad mood\",\n",
        "    \"AvPD\": \"Self-loathing\",\n",
        "    \"SelfHate\": \"Self-loathing\",\n",
        "    \"selfhelp\": \"Self-loathing\",\n",
        "    \"socialanxiety\": \"Self-loathing\",\n",
        "    \"whatsbotheringyou\": \"Self-loathing\",\n",
        "    \"insomnia\": \"Sleep problem\",\n",
        "    \"sleep\": \"Sleep problem\",\n",
        "    \"cfs\": \"Somatic complaint\",\n",
        "    \"ChronicPain\": \"Somatic complaint\",\n",
        "    \"Constipation\": \"Somatic complaint\",\n",
        "    \"EssentialTremor\": \"Somatic complaint\",\n",
        "    \"headaches\": \"Somatic complaint\",\n",
        "    \"ibs\": \"Somatic complaint\",\n",
        "    \"tinnitus\": \"Somatic complaint\",\n",
        "    \"AdultSelfHarm\": \"Suicidal thoughts and attempts\",\n",
        "    \"selfharm\": \"Suicidal thoughts and attempts\",\n",
        "    \"SuicideWatch\": \"Suicidal thoughts and attempts\",\n",
        "    \"Guilt\": \"Worthlessness\",\n",
        "    \"Pessimism\": \"Worthlessness\",\n",
        "    \"selfhelp\": \"Worthlessness\",\n",
        "    \"whatsbotheringyou\": \"Worthlessness\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "Y-qSHKKFz1RR"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Preprocessing function\n",
        "def tokenize(symptom_data, control_data):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  Posts from a specified symptom (or all symptoms) and control posts\n",
        "\n",
        "  Function:\n",
        "  Tokenize all symptom and control posts using happiestfuntokenizer\n",
        "\n",
        "  Returns:\n",
        "  Tokenized symptom and control posts\n",
        "  \"\"\"\n",
        "\n",
        "  # preserve_keywords=True because reddit posts are likely to have lots of keywords\n",
        "  #   like hashtags and @[person], and I think it's best to keep those together\n",
        "  tokenizer = Tokenizer(preserve_keywords=True)\n",
        "\n",
        "  # Tokenizing posts from both symptom and control datasets\n",
        "  symptom_tokens = [tokenizer.tokenize(post) for post in symptom_data]\n",
        "  control_tokens = [tokenizer.tokenize(post) for post in control_data]\n",
        "\n",
        "  return symptom_tokens, control_tokens"
      ],
      "metadata": {
        "id": "MWGVUju_WxuP"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stop_words(symptom_tokens, control_tokens):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  Tokenized symptom and control posts\n",
        "\n",
        "  Function:\n",
        "  Find and remove the top 100 most frequent words\n",
        "\n",
        "  Returns:\n",
        "  Filtered tokenizes symptom and control posts\n",
        "  \"\"\"\n",
        "  # Find top 100 most common words in control tokens\n",
        "  word_counter_list = list(chain.from_iterable(control_tokens))\n",
        "  common_words = {word for word, _ in Counter(word_counter_list).most_common(100)}\n",
        "\n",
        "  # Filter out common words from the depression and control datasets\n",
        "  symptom_tokens_filtered = [\n",
        "      [token for token in post if token not in common_words] for post in symptom_tokens\n",
        "  ]\n",
        "  control_tokens_filtered = [\n",
        "      [token for token in post if token not in common_words] for post in control_tokens\n",
        "  ]\n",
        "\n",
        "  return symptom_tokens_filtered, control_tokens_filtered"
      ],
      "metadata": {
        "id": "Q3j9z7UuW3eG"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit Topics with LDA\n",
        "\n",
        " - Don't use MALLET (as the paper does), use some other LDA implementation."
      ],
      "metadata": {
        "id": "U4I37U1SXAEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaMulticore\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "def train_lda_on_full_data(symptom_data, control_data):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  Raw symptom and control posts\n",
        "\n",
        "  Function:\n",
        "  Train LDA model on the full dataset (control + depression)\n",
        "\n",
        "  Returns:\n",
        "  LDA model, total dictionary, total corpus\n",
        "  \"\"\"\n",
        "\n",
        "  # Tokenize the symptom and control datasets\n",
        "  symptom_tokens, control_tokens = tokenize(symptom_data, control_data)\n",
        "\n",
        "  # Remove stopwords from both datasets\n",
        "  symptom_tokens_filtered, control_tokens_filtered = stop_words(symptom_tokens, control_tokens)\n",
        "\n",
        "  # Combine the tokenized data (symptom + control)\n",
        "  combined_tokens = symptom_tokens_filtered + control_tokens_filtered\n",
        "\n",
        "  # Create a Gensim dictionary and corpus\n",
        "  dictionary = Dictionary(combined_tokens)\n",
        "  corpus = [dictionary.doc2bow(text) for text in combined_tokens]\n",
        "\n",
        "  # Train the LDA model\n",
        "  lda_model = LdaMulticore(corpus, num_topics=200, id2word=dictionary)\n",
        "\n",
        "  # save LDA model, dictionary, corpus\n",
        "  pickle.dump(lda_model, open(f'{FILEPATH}/lda_model.pkl', 'wb'))\n",
        "  pickle.dump(dictionary, open(f'{FILEPATH}/lda_dictionary.pkl', 'wb'))\n",
        "  pickle.dump(corpus, open(f'{FILEPATH}/lda_corpus.pkl', 'wb'))\n",
        "\n",
        "  return lda_model, dictionary, corpus"
      ],
      "metadata": {
        "id": "xf3surfWXH-q"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zoom in on Dictionary and Corpus"
      ],
      "metadata": {
        "id": "yVyjdnhT_xnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize inputs\n",
        "data = load(\"reddit_depression_data\")\n",
        "data1 = dataset_generation(data, subreddit_category_mapping)\n",
        "symptom_posts = data1[data1['Category'] == \"Anger\"]['text'] # Using Anger as an example\n",
        "control_posts = data1[data1['Category'] == \"Control\"]['text']\n",
        "symptom_tokens, control_tokens = tokenize(symptom_posts, control_posts)\n",
        "symptom_tokens_filtered, control_tokens_filtered = stop_words(symptom_tokens, control_tokens)\n",
        "combined_tokens = symptom_tokens_filtered + control_tokens_filtered\n",
        "\n",
        "# Create Gensim dictionary\n",
        "dictionary = Dictionary(combined_tokens)\n",
        "print(dictionary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0IO-r2khrt0",
        "outputId": "51416d6e-d757-412d-cecc-6720d94decfd"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary<24106 unique tokens: ['advice', 'after', 'again', 'always', 'anger']...>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00ibOQuL95fD",
        "outputId": "c56ecddc-8a5b-4e03-90eb-8b2a15423fe8"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24106"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "gR-Yd9x5-xZ7",
        "outputId": "df595f4b-0d90-4014-d1be-2b1c4208e055"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gensim.corpora.dictionary.Dictionary"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>gensim.corpora.dictionary.Dictionary</b><br/>def __init__(documents=None, prune_at=2000000)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/gensim/corpora/dictionary.py</a>Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
              "\n",
              "Notable instance attributes:\n",
              "\n",
              "Attributes\n",
              "----------\n",
              "token2id : dict of (str, int)\n",
              "    token -&gt; token_id. I.e. the reverse mapping to `self[token_id]`.\n",
              "cfs : dict of (int, int)\n",
              "    Collection frequencies: token_id -&gt; how many instances of this token are contained in the documents.\n",
              "dfs : dict of (int, int)\n",
              "    Document frequencies: token_id -&gt; how many documents contain this token.\n",
              "num_docs : int\n",
              "    Number of documents processed.\n",
              "num_pos : int\n",
              "    Total number of corpus positions (number of processed words).\n",
              "num_nnz : int\n",
              "    Total number of non-zeroes in the BOW matrix (sum of the number of unique\n",
              "    words per document over the entire corpus).</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 21);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bqmrhK4l--L8",
        "outputId": "be74c777-c671-4490-bf55-a6e904062b18"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'advice'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary[2404]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dfy_3FO498lE",
        "outputId": "c96efbbe-177b-410c-e826-6dd738b9a40e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pocket'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id, token in dictionary.items():\n",
        "  if token_id == 10:\n",
        "    break\n",
        "  print(f\"ID: {token_id}, Token: {token}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG4vSXy6-f3s",
        "outputId": "5137e447-92c6-4001-f43b-e214b4dfdd48"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: 0, Token: advice\n",
            "ID: 1, Token: after\n",
            "ID: 2, Token: again\n",
            "ID: 3, Token: always\n",
            "ID: 4, Token: anger\n",
            "ID: 5, Token: angry\n",
            "ID: 6, Token: anyone\n",
            "ID: 7, Token: avoid\n",
            "ID: 8, Token: barking\n",
            "ID: 9, Token: becoming\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_combined_tokens = [\n",
        "    ['anger', 'feel', 'always'],\n",
        "    ['after', 'anger', 'advice', 'advice']\n",
        "]\n",
        "\n",
        "example_dictionary = Dictionary(example_combined_tokens)\n",
        "print(example_dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL9Ze6cw_Nz7",
        "outputId": "79f00c7f-24b5-41a5-bf78-e366706e3d29"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary<5 unique tokens: ['always', 'anger', 'feel', 'advice', 'after']>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create corpus from dictionary\n",
        "corpus = [dictionary.doc2bow(text) for text in combined_tokens]\n"
      ],
      "metadata": {
        "id": "e0_YRL3aAF2r"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7Imo8mECoKV",
        "outputId": "d1becc52-805e-4b99-846b-9ab48822d3aa"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1k6K2EEAdbZ",
        "outputId": "a9fa55b5-9256-4bb2-cd00-962073b5bbfa"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 3), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 3), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 2), (41, 2), (42, 3), (43, 1), (44, 1), (45, 2), (46, 3)], [(4, 1), (5, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1)], [(58, 1), (59, 1), (60, 1), (61, 1), (62, 1)], [(0, 1), (33, 1), (60, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 3), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 1), (80, 2), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 2), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 2), (98, 1), (99, 1), (100, 1), (101, 1)], [(61, 1), (102, 1), (103, 1), (104, 1)], [(1, 1), (3, 3), (4, 3), (5, 5), (6, 2), (14, 3), (21, 3), (30, 1), (33, 1), (34, 1), (38, 2), (42, 1), (51, 1), (55, 1), (56, 1), (60, 1), (67, 3), (85, 2), (86, 2), (89, 1), (96, 1), (97, 3), (104, 1), (105, 9), (106, 1), (107, 1), (108, 1), (109, 1), (110, 2), (111, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 2), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 2), (151, 1), (152, 1), (153, 2), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 2), (172, 1), (173, 1), (174, 2), (175, 3), (176, 4), (177, 1), (178, 1), (179, 2), (180, 1), (181, 1), (182, 1), (183, 1), (184, 6), (185, 1), (186, 1), (187, 2), (188, 3), (189, 1), (190, 1), (191, 1), (192, 4), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 2), (204, 1), (205, 1), (206, 3), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 6), (221, 1), (222, 3), (223, 1), (224, 2), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 2), (234, 1), (235, 3), (236, 1), (237, 1), (238, 1), (239, 1), (240, 3), (241, 1), (242, 1), (243, 1), (244, 1), (245, 3), (246, 3), (247, 1), (248, 1), (249, 2), (250, 3), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 1), (282, 3), (283, 4), (284, 1), (285, 3), (286, 1), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), (295, 2), (296, 1), (297, 2), (298, 1), (299, 2), (300, 1), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 2), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1), (316, 1)], [(61, 1), (100, 1), (203, 1), (297, 1), (317, 1), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1)], [(0, 1), (3, 2), (14, 1), (68, 1), (117, 1), (125, 1), (135, 2), (176, 1), (184, 3), (209, 1), (287, 1), (297, 2), (323, 2), (324, 1), (325, 1), (326, 1), (327, 1), (328, 2), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1)], [(3, 2), (5, 4), (6, 1), (22, 1), (23, 1), (24, 1), (27, 1), (39, 1), (54, 2), (56, 2), (57, 1), (67, 1), (102, 5), (104, 1), (120, 1), (176, 1), (186, 1), (188, 1), (191, 2), (224, 1), (236, 2), (245, 1), (246, 1), (287, 1), (295, 1), (330, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 2), (347, 1), (348, 1), (349, 1), (350, 1), (351, 2), (352, 1), (353, 1), (354, 1), (355, 1), (356, 1), (357, 1), (358, 1), (359, 1), (360, 1), (361, 1), (362, 1), (363, 1), (364, 1), (365, 1), (366, 1), (367, 1), (368, 1), (369, 1), (370, 1), (371, 6), (372, 1), (373, 1), (374, 2), (375, 2), (376, 1), (377, 1), (378, 1), (379, 1), (380, 1), (381, 1), (382, 1), (383, 1), (384, 2), (385, 1), (386, 3), (387, 1), (388, 1), (389, 1), (390, 1), (391, 1), (392, 2), (393, 3), (394, 1), (395, 1), (396, 1), (397, 1), (398, 1), (399, 2), (400, 1), (401, 3), (402, 1), (403, 1), (404, 1), (405, 1), (406, 1), (407, 13), (408, 1)], [(4, 4), (30, 1), (39, 3), (40, 1), (47, 1), (51, 1), (54, 1), (67, 1), (77, 1), (78, 2), (119, 1), (133, 2), (171, 1), (203, 4), (208, 2), (236, 2), (281, 1), (283, 1), (286, 1), (297, 1), (313, 1), (322, 1), (323, 1), (330, 1), (345, 1), (409, 2), (410, 1), (411, 1), (412, 1), (413, 1), (414, 1), (415, 2), (416, 1), (417, 1), (418, 1), (419, 1), (420, 1), (421, 1), (422, 1), (423, 1), (424, 1), (425, 1), (426, 1), (427, 1), (428, 2), (429, 1), (430, 1), (431, 1), (432, 1), (433, 1), (434, 1), (435, 1), (436, 1), (437, 1), (438, 1), (439, 1), (440, 1), (441, 1), (442, 1), (443, 1), (444, 1), (445, 1), (446, 1), (447, 2), (448, 1), (449, 1), (450, 1), (451, 2), (452, 1), (453, 3), (454, 1), (455, 2), (456, 1), (457, 1), (458, 1), (459, 1), (460, 2), (461, 1), (462, 1), (463, 1), (464, 2), (465, 1), (466, 2), (467, 1), (468, 1), (469, 2), (470, 1), (471, 5)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsQ-lBAG9Bs2",
        "outputId": "ced5b7f5-6b91-4fcf-9187-40fe8b7002b5"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4924"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(symptom_posts), len(control_posts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdT4we9j9Es9",
        "outputId": "7fe39981-d7f1-4539-9410-8681a454071d"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(555, 4369)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(symptom_posts) + len(control_posts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEplHYe-9Jrd",
        "outputId": "0f0775b0-8fbe-4fb8-8d02-791e0f376f53"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4924"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjFQUS-K9Ofi",
        "outputId": "8cefc5a7-91d6-4fae-cd6d-3ea84edf0679"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 2),\n",
              " (1, 1),\n",
              " (2, 1),\n",
              " (3, 1),\n",
              " (4, 3),\n",
              " (5, 2),\n",
              " (6, 1),\n",
              " (7, 1),\n",
              " (8, 1),\n",
              " (9, 1),\n",
              " (10, 1),\n",
              " (11, 1),\n",
              " (12, 1),\n",
              " (13, 1),\n",
              " (14, 2),\n",
              " (15, 1),\n",
              " (16, 3),\n",
              " (17, 1),\n",
              " (18, 1),\n",
              " (19, 1),\n",
              " (20, 1),\n",
              " (21, 1),\n",
              " (22, 1),\n",
              " (23, 1),\n",
              " (24, 1),\n",
              " (25, 1),\n",
              " (26, 1),\n",
              " (27, 2),\n",
              " (28, 1),\n",
              " (29, 1),\n",
              " (30, 1),\n",
              " (31, 1),\n",
              " (32, 1),\n",
              " (33, 2),\n",
              " (34, 1),\n",
              " (35, 1),\n",
              " (36, 1),\n",
              " (37, 1),\n",
              " (38, 1),\n",
              " (39, 2),\n",
              " (40, 2),\n",
              " (41, 2),\n",
              " (42, 3),\n",
              " (43, 1),\n",
              " (44, 1),\n",
              " (45, 2),\n",
              " (46, 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQdYNVfjAt4q",
        "outputId": "ade368bc-d8bd-49fb-efb3-1b16e6764f18"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4, 1),\n",
              " (5, 1),\n",
              " (47, 1),\n",
              " (48, 1),\n",
              " (49, 1),\n",
              " (50, 1),\n",
              " (51, 1),\n",
              " (52, 1),\n",
              " (53, 1),\n",
              " (54, 1),\n",
              " (55, 1),\n",
              " (56, 1),\n",
              " (57, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_combined_tokens = [\n",
        "    ['anger', 'feel', 'always'],\n",
        "    ['after', 'anger', 'advice', 'advice']\n",
        "]\n",
        "\n",
        "example_dictionary = Dictionary(example_combined_tokens)\n",
        "\n",
        "example_corpus = [example_dictionary.doc2bow(text) for text in example_combined_tokens]\n",
        "print(example_dictionary)\n",
        "print(example_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD3W-g-wBo4X",
        "outputId": "d1f12016-e807-4517-b1ca-34f69322ad24"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary<5 unique tokens: ['always', 'anger', 'feel', 'advice', 'after']>\n",
            "[[(0, 1), (1, 1), (2, 1)], [(1, 1), (3, 2), (4, 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id, token in example_dictionary.items():\n",
        "    print(f\"ID: {token_id}, Token: {token}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oebiGTwHCe_j",
        "outputId": "def2f7f0-ae4a-400f-f3b4-dad47af916b2"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: 0, Token: always\n",
            "ID: 1, Token: anger\n",
            "ID: 2, Token: feel\n",
            "ID: 3, Token: advice\n",
            "ID: 4, Token: after\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfH9UaDLB6Hd",
        "outputId": "f26c6809-de64-433f-e7f1-42aeedcdcf64"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 1), (1, 1), (2, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_corpus[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBhu_QMoCFH9",
        "outputId": "90a82751-cc59-41e9-891c-52c209641b1f"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 1), (3, 2), (4, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End code discussion"
      ],
      "metadata": {
        "id": "Ag9DKEflDKov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_distributions_for_posts(lda_model, corpus):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  Trained LDA model, corpus of specific symptom + all control posts\n",
        "\n",
        "  Function:\n",
        "  Gets the topic distributions for each symptom and control post in a given corpus\n",
        "\n",
        "  Returns:\n",
        "  Matrix of topic distributions for each post\n",
        "\n",
        "  \"\"\"\n",
        "  # Infers the topic distribution for each post in the corpus based on the\n",
        "  #   word-topic probabilities the model has learned.\n",
        "  #   Minimum_probability=0.0 to make sure each post has 200 topic items even if the\n",
        "  #   probability of that topic is 0. This is to ensure no missing datapoints when making matrix\n",
        "  topic_distributions = [lda_model.get_document_topics(post, minimum_probability=0.0) for post in corpus]\n",
        "\n",
        "  # Convert topic_distributions to a matrix\n",
        "  topic_distributions = np.array([[topic_prob for _, topic_prob in doc] for doc in topic_distributions])\n",
        "  return topic_distributions\n"
      ],
      "metadata": {
        "id": "qrfVAJgUO3Hs"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lda_data_for_symptom(symptom_posts, control_posts, lda_model, dictionary):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  Raw symptom and control posts, trained LDA model on full dataset, dictionary of full dataset\n",
        "\n",
        "  Function:\n",
        "  Gets the topic distributions for each post in single symptom vs control data\n",
        "\n",
        "  Returns:\n",
        "  X_lda as the distribution of topics in each post and labels as whether the post was from\n",
        "  the depression symptom posts (1) or the control posts (0)\n",
        "  \"\"\"\n",
        "\n",
        "  # Tokenize the symptom and control datasets\n",
        "  symptom_tokens, control_tokens = tokenize(symptom_posts, control_posts)\n",
        "\n",
        "  # Remove stopwords from both datasets\n",
        "  symptom_tokens_filtered, control_tokens_filtered = stop_words(symptom_tokens, control_tokens)\n",
        "\n",
        "  # Combine the tokenized data (symptom + control)\n",
        "  combined_tokens = symptom_tokens_filtered + control_tokens_filtered\n",
        "\n",
        "  # Create the corpus for the LDA model\n",
        "  corpus = [dictionary.doc2bow(text) for text in combined_tokens]\n",
        "\n",
        "  # Generate label 1 for symptom, 0 for control\n",
        "  labels = np.concatenate([np.ones(len(symptom_posts)), np.zeros(len(control_posts))])\n",
        "\n",
        "  # Generate LDA topic distributions for these posts\n",
        "  X_lda = get_topic_distributions_for_posts(lda_model, corpus)\n",
        "\n",
        "  return X_lda, labels\n"
      ],
      "metadata": {
        "id": "kTiBHJ_nVJEg"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoBERTa Embeddings"
      ],
      "metadata": {
        "id": "E0-97hsVXNkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Your RoBERTa code!\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "def extract_roberta_features(symptom_posts, control_posts, model_name='distilroberta-base', batch_size=16, layer=5):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  Raw (specific) symptom and control posts, model name (deafult='distilroberta-base'),\n",
        "  batch_size (default=16), layer (default=5)\n",
        "\n",
        "  Function:\n",
        "  Loads DistilRoBERTa tokenizer and model, batches and tokenizes raw posts before\n",
        "  feeding them to the model. Extracts and averages hidden state embeddings from layer 5\n",
        "  and stacks them.\n",
        "\n",
        "  Returns:\n",
        "  features - np.array of DistilRoBERTa embeddings per post,\n",
        "  labels - np.array where 1=symptom and 0=control\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Load tokenizer and model\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "\n",
        "  # set model to eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # put model on cuda\n",
        "  model.to(\"cuda\")\n",
        "\n",
        "  # Combine raw symptom and control posts\n",
        "  combined_text = symptom_posts.tolist() + control_posts.tolist()\n",
        "\n",
        "  features = []\n",
        "  # loop through combined text 16 posts at a time\n",
        "  for i in range(0, len(combined_text), batch_size):\n",
        "      # create batched slice from combined text\n",
        "      batch_texts = combined_text[i:i+batch_size]\n",
        "\n",
        "      # pass batch of posts to DistilRoBERTa tokenizer. Include padding and trunctation\n",
        "      # Padding is used to extend shorter batches with a pad token, so every batch is the same length.\n",
        "      #   This prevents errors from passing differnt shaped batches to the model\n",
        "      # Truncation is used because it ensures that the inputted tokens don't exceed the 512 limit of\n",
        "      #   RoBERTa. If the batch does exceed, then it will be clipped to the first 512 tokens\n",
        "      tokens = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "      # Set tokens to \"cuda\" to allow model to process them using a GPU\n",
        "      tokens = {key: val.to(\"cuda\") for key, val in tokens.items()}\n",
        "\n",
        "      # Set model to no_grad to prevent backpropgation\n",
        "      with torch.no_grad():\n",
        "          outputs = model(**tokens)\n",
        "\n",
        "      # Extract embeddings from the specified layer\n",
        "      hidden_states = outputs.hidden_states[layer]\n",
        "\n",
        "      # averages hidden state token embeddings from layer 5 to give shape(embedding,) for each post\n",
        "      batch_features = hidden_states.mean(dim=1).cpu().numpy()\n",
        "      features.append(batch_features)\n",
        "\n",
        "  # Combine all batches\n",
        "  features = np.vstack(features)\n",
        "\n",
        "  # Generate label 1 for symptom, 0 for control\n",
        "  labels = np.concatenate([np.ones(len(symptom_posts)), np.zeros(len(control_posts))])\n",
        "\n",
        "  return features, labels"
      ],
      "metadata": {
        "id": "blx1SWVMXYDp"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation"
      ],
      "metadata": {
        "id": "dH5sG9_u_nXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation(X, y):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  X - features, y - labels\n",
        "\n",
        "  Function:\n",
        "  Perform 5-fold cross validation with random forest to evaluate LDA topic distributions or\n",
        "  RoBERTa embedding performance on predicting symptom vs control.\n",
        "\n",
        "  Returns:\n",
        "  cross-validation results across 5 folds\n",
        "  \"\"\"\n",
        "\n",
        "  rf_classifier = RandomForestClassifier()\n",
        "  cv = KFold(n_splits=5, shuffle=True)\n",
        "  results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "  return results"
      ],
      "metadata": {
        "id": "3dU_WIhB_sJL"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "rDWxuF2jXtwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "data = load(\"reddit_depression_data\")"
      ],
      "metadata": {
        "id": "0M5yQvviY86G"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(data):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "  data - loaded from reddit depression dataset pickle\n",
        "\n",
        "  Function:\n",
        "  Computes AUC scores of each symptom for LDA probability distributions and DistilRoBERTa\n",
        "  embeddings using a random forest classier and 5-fold cross-validation.\n",
        "\n",
        "  Returns:\n",
        "  Table of summary AUC results\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Add category and label features\n",
        "  data = dataset_generation(data, subreddit_category_mapping)\n",
        "\n",
        "  # Preloaded lda model and dictionary from this call:\n",
        "  #   lda_model, dictionary, corpus = train_lda_on_full_data(data[data['Label'] == 1]['text'], data[data[\"Label\"] == 0]['text'])\n",
        "  lda_model_preloaded = pd.read_pickle(f'{FILEPATH}/lda_model.pkl')\n",
        "  lda_dictionary_preloaded = pd.read_pickle(f'{FILEPATH}/lda_dictionary.pkl')\n",
        "\n",
        "  # Initialize dictionaries to hold AUC info\n",
        "  LDA_AUC = {}\n",
        "  ROBERTA_AUC = {}\n",
        "\n",
        "  # Loop through each symptom\n",
        "  for symptom in depression_symptoms:\n",
        "    # Filter the dataset for the current symptom (and control)\n",
        "    print(f\"{symptom} vs control\")\n",
        "    symptom_posts = data[data['Category'] == symptom]['text']\n",
        "    control_posts = data[data['Category'] == \"Control\"]['text']\n",
        "\n",
        "    print(\"Running LDA...\")\n",
        "    # Prepare the LDA data (topic distributions and labels)\n",
        "    X_lda, y = prepare_lda_data_for_symptom(symptom_posts, control_posts, lda_model_preloaded, lda_dictionary_preloaded)\n",
        "\n",
        "    # run cv on LDA topic distributions\n",
        "    lda_auc = cross_validation(X_lda, y)\n",
        "    LDA_AUC[symptom] = lda_auc\n",
        "\n",
        "    print(\"Running DistilRoBERTa\")\n",
        "    # Prepare the LDA data (topic distributions and labels)\n",
        "    X_embeddings, y = extract_roberta_features(symptom_posts, control_posts)\n",
        "\n",
        "    # run cv on LDA topic distributions\n",
        "    roberta_auc = cross_validation(X_embeddings, y)\n",
        "    ROBERTA_AUC[symptom] = roberta_auc\n",
        "\n",
        "  print(\"Finished!\")\n",
        "  print(\"Displaying results\")\n",
        "  # Display LDA and RoBERTa AUCs per symptom\n",
        "  LDA_scores = []\n",
        "  ROBERTA_scores = []\n",
        "\n",
        "  # Create dataframe to visualize AUC for LDA and DistilRoBERTa per symptom\n",
        "  for symptom in LDA_AUC:\n",
        "    LDA_scores.append(np.mean(LDA_AUC[symptom]['test_score']))\n",
        "    ROBERTA_scores.append(np.mean(ROBERTA_AUC[symptom]['test_score']))\n",
        "\n",
        "  scores = {\"Symptoms\": depression_symptoms,\n",
        "            \"LDA\": LDA_scores,\n",
        "            \"DistilRoBERTa\": ROBERTA_scores}\n",
        "\n",
        "  scores_df = pd.DataFrame(scores)\n",
        "  print(scores_df)\n",
        "  scores_df.to_csv(f\"{FILEPATH}/LDA_vs_DistilRoBERTa_scores.csv\")"
      ],
      "metadata": {
        "id": "koTBPhcDXujb"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(data)"
      ],
      "metadata": {
        "id": "x2QsTGq5eu2B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7c9c0caa9d5a4de5b664b9c03f423567",
            "e080b6797c85434ab49bdf43ef66922e",
            "9fc45e533316416bbbecd87c51cd578d",
            "7534a65fa8a9446da84044ea31f35d70",
            "cda7966e5b5d4933b17ffddaa13dc366",
            "7d5614b0421b4c729f686060d01bed4c",
            "e702f1aa7c044715bdd0a344d08e3a6e",
            "cfaffc4b93b34198bedb0eb8828a6700",
            "24765fecce394e3b90e3bc8732a6571b",
            "43f5d2e4cf1d451d934ab62be5479546",
            "abe56ebd696849e9944d4b41d7801ff7",
            "b21729ef4e9b4f47b15d8a331404ebf9",
            "c0314a47a1094752b75817d3c390e717",
            "24cd71c71a294c9b91f0eaf8b5c589b2",
            "d7796889e7664f8b9ed736dacec61119",
            "78f9ba1aa8a246de8967ac4ca0e1207f",
            "1cf3f4fa4c4848a4a37a137829ae27a3",
            "4d84e176f4294792957a1d7949d09984",
            "287f574074ad4f2c91b0504d3af466e6",
            "f1b9499cd77a43aab2d0e56a45ac3d21",
            "7506189fd91a4907a7290bc3265a9be0",
            "583ac12e547840968d117c40257b6423",
            "f99a1c7dbf7a4899b042c9fe8efd7829",
            "14cb69554c7c4365a1d55163656788bb",
            "3be9beebddb748e49daea7eeea4c4018",
            "666a99e531dd4b31bbcae397304e6f4f",
            "c483d50b83a84c84ab5f67069c508641",
            "8fc89ea093ec431a8ecd76c5dba07a24",
            "1980f73b8e5a4f688dbe408d8db7de21",
            "fdad5c4799624c86af41294b830cf4c4",
            "88e9600e26254f188b27d9bbea7e0e1e",
            "db99dd4e083142c3949776da48859187",
            "9543af0c48db4085af310a2d57676fb9",
            "0265859e5c7d49db82f38c4e4ae5c11a",
            "e6ce7d9688f94f6c84643eee511d4274",
            "b9cbe76609724873bcba4d4833f62931",
            "96e9b79b4a0a4758bef5e802c35604bb",
            "32f62f21d8214b8f90923b6e4adde6a3",
            "f887378a0574495da34cfa6e62de4b8a",
            "999031dacdbd4da0b887a97d3ff6e533",
            "869b42a8d2fa483a86124d91ebf36942",
            "b5078786cadf430293549c3c800bcac1",
            "263a834e30c24c5a84707843aa62bfd1",
            "8e74bca1fa2f4b8ba52d8364496e3fb7",
            "88720ee75100422484745a80846358c3",
            "61c0e322b62547e2817c47811a103351",
            "03690b53bc074d478ae38106cbf6a628",
            "c688e67ff8594b6480c2a0feea850af6",
            "8e13a1009a8646359fd799fdb6e299ad",
            "dfd2a32a0a8f42b5918427fd4bee49a3",
            "193be27d7f094e6cadc423b7c4255498",
            "a8e02588f4314284b35c6e790c53a4d0",
            "42594735e9e147538efa802f05346420",
            "173c88cf05ae4fe2936673ef9504d511",
            "9e56fb891b874f7aa7ec497050349e92",
            "1e77bf0416d64f57a3ca56bcef36e533",
            "015c7ffa60294b0ea416608d73d2d747",
            "4729c709b1524f54add8c38960786353",
            "a9237f6980c04e599b99427185811d9c",
            "f5fb455f34574a288798cebfa55b7d12",
            "b315f64fc2154e2c9a9efb5d77880d9d",
            "c92ff53fbfa44e7789f975b43b121885",
            "ed27a7682b994aa18c08d2ba4f78dabe",
            "9d7665f2ef8644c593261a1e111696e8",
            "9a8c3bf9bc294705ba508870a14fe99c",
            "fc476fe92875404dbaca3af5c7272796"
          ]
        },
        "collapsed": true,
        "outputId": "6420bfbf-54ee-4b7f-a840-7cb688feb778"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anger vs control\n",
            "Running LDA...\n",
            "Running DistilRoBERTa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c9c0caa9d5a4de5b664b9c03f423567"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b21729ef4e9b4f47b15d8a331404ebf9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f99a1c7dbf7a4899b042c9fe8efd7829"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0265859e5c7d49db82f38c4e4ae5c11a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88720ee75100422484745a80846358c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e77bf0416d64f57a3ca56bcef36e533"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anhedonia vs control\n",
            "Running LDA...\n",
            "Running DistilRoBERTa\n",
            "Anxiety vs control\n",
            "Running LDA...\n",
            "Running DistilRoBERTa\n",
            "Disordered eating vs control\n",
            "Running LDA...\n",
            "Running DistilRoBERTa\n",
            "Loneliness vs control\n",
            "Running LDA...\n",
            "Running DistilRoBERTa\n",
            "Sad mood vs control\n",
            "Running LDA...\n",
            "Running DistilRoBERTa\n",
            "Self-loathing vs control\n",
            "Running LDA...\n",
            "Running DistilRoBERTa\n",
            "Sleep problem vs control\n",
            "Running LDA...\n",
            "Running DistilRoBERTa\n",
            "Somatic complaint vs control\n",
            "Running LDA...\n",
            "Running DistilRoBERTa\n",
            "Worthlessness vs control\n",
            "Running LDA...\n",
            "Running DistilRoBERTa\n",
            "Finished!\n",
            "Displaying results\n",
            "            Symptoms       LDA  DistilRoBERTa\n",
            "0              Anger  0.930740       0.946597\n",
            "1          Anhedonia  0.958075       0.958202\n",
            "2            Anxiety  0.943000       0.956876\n",
            "3  Disordered eating  0.952881       0.954538\n",
            "4         Loneliness  0.870590       0.917033\n",
            "5           Sad mood  0.842127       0.935559\n",
            "6      Self-loathing  0.919082       0.942580\n",
            "7      Sleep problem  0.974569       0.959686\n",
            "8  Somatic complaint  0.918264       0.931319\n",
            "9      Worthlessness  0.756143       0.922153\n"
          ]
        }
      ]
    }
  ]
}
